{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-ce0da3b3913b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0.9191\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# TensorFlow 라이브러리를 추가한다.\n",
    "import tensorflow as tf\n",
    "\n",
    "# 변수들을 설정한다.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# cross-entropy 모델을 설정한다.\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# 경사하강법으로 모델을 학습한다.\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# 학습된 모델이 얼마나 정확한지를 출력한다.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Step :     0\tLoss: 2.153\tAcc: 39.00%\n",
      "Step :   100\tLoss: 0.678\tAcc: 82.00%\n",
      "Step :   200\tLoss: 0.510\tAcc: 86.00%\n",
      "Step :   300\tLoss: 0.532\tAcc: 89.00%\n",
      "Step :   400\tLoss: 0.524\tAcc: 86.00%\n",
      "Step :   500\tLoss: 0.428\tAcc: 88.00%\n",
      "Step :   600\tLoss: 0.236\tAcc: 97.00%\n",
      "Step :   700\tLoss: 0.382\tAcc: 91.00%\n",
      "Step :   800\tLoss: 0.343\tAcc: 91.00%\n",
      "Step :   900\tLoss: 0.373\tAcc: 89.00%\n",
      "Step :  1000\tLoss: 0.356\tAcc: 91.00%\n",
      "Step :  1100\tLoss: 0.269\tAcc: 94.00%\n",
      "Step :  1200\tLoss: 0.252\tAcc: 94.00%\n",
      "Step :  1300\tLoss: 0.385\tAcc: 91.00%\n",
      "Step :  1400\tLoss: 0.386\tAcc: 91.00%\n",
      "Step :  1500\tLoss: 0.412\tAcc: 89.00%\n",
      "Step :  1600\tLoss: 0.321\tAcc: 92.00%\n",
      "Step :  1700\tLoss: 0.376\tAcc: 88.00%\n",
      "Step :  1800\tLoss: 0.214\tAcc: 95.00%\n",
      "Step :  1900\tLoss: 0.310\tAcc: 93.00%\n",
      "Step :  2000\tLoss: 0.423\tAcc: 91.00%\n",
      "Step :  2100\tLoss: 0.422\tAcc: 88.00%\n",
      "Step :  2200\tLoss: 0.278\tAcc: 92.00%\n",
      "Step :  2300\tLoss: 0.262\tAcc: 92.00%\n",
      "Step :  2400\tLoss: 0.277\tAcc: 92.00%\n",
      "Step :  2500\tLoss: 0.316\tAcc: 91.00%\n",
      "Step :  2600\tLoss: 0.352\tAcc: 89.00%\n",
      "Step :  2700\tLoss: 0.289\tAcc: 92.00%\n",
      "Step :  2800\tLoss: 0.230\tAcc: 94.00%\n",
      "Step :  2900\tLoss: 0.303\tAcc: 90.00%\n",
      "Step :  3000\tLoss: 0.216\tAcc: 92.00%\n",
      "Step :  3100\tLoss: 0.427\tAcc: 86.00%\n",
      "Step :  3200\tLoss: 0.292\tAcc: 94.00%\n",
      "Step :  3300\tLoss: 0.321\tAcc: 90.00%\n",
      "Step :  3400\tLoss: 0.367\tAcc: 88.00%\n",
      "Step :  3500\tLoss: 0.247\tAcc: 93.00%\n",
      "Step :  3600\tLoss: 0.263\tAcc: 93.00%\n",
      "Step :  3700\tLoss: 0.453\tAcc: 91.00%\n",
      "Step :  3800\tLoss: 0.282\tAcc: 93.00%\n",
      "Step :  3900\tLoss: 0.398\tAcc: 89.00%\n",
      "Step :  4000\tLoss: 0.442\tAcc: 88.00%\n",
      "Step :  4100\tLoss: 0.233\tAcc: 93.00%\n",
      "Step :  4200\tLoss: 0.360\tAcc: 90.00%\n",
      "Step :  4300\tLoss: 0.260\tAcc: 91.00%\n",
      "Step :  4400\tLoss: 0.215\tAcc: 91.00%\n",
      "Step :  4500\tLoss: 0.255\tAcc: 95.00%\n",
      "Step :  4600\tLoss: 0.249\tAcc: 93.00%\n",
      "Step :  4700\tLoss: 0.383\tAcc: 92.00%\n",
      "Step :  4800\tLoss: 0.189\tAcc: 94.00%\n",
      "Step :  4900\tLoss: 0.324\tAcc: 93.00%\n",
      "Step :  5000\tLoss: 0.383\tAcc: 87.00%\n",
      "Step :  5100\tLoss: 0.368\tAcc: 90.00%\n",
      "Step :  5200\tLoss: 0.175\tAcc: 95.00%\n",
      "Step :  5300\tLoss: 0.220\tAcc: 93.00%\n",
      "Step :  5400\tLoss: 0.264\tAcc: 93.00%\n",
      "Step :  5500\tLoss: 0.231\tAcc: 95.00%\n",
      "Step :  5600\tLoss: 0.355\tAcc: 90.00%\n",
      "Step :  5700\tLoss: 0.403\tAcc: 90.00%\n",
      "Step :  5800\tLoss: 0.298\tAcc: 91.00%\n",
      "Step :  5900\tLoss: 0.231\tAcc: 91.00%\n",
      "Step :  6000\tLoss: 0.229\tAcc: 89.00%\n",
      "Step :  6100\tLoss: 0.302\tAcc: 93.00%\n",
      "Step :  6200\tLoss: 0.218\tAcc: 92.00%\n",
      "Step :  6300\tLoss: 0.271\tAcc: 95.00%\n",
      "Step :  6400\tLoss: 0.481\tAcc: 85.00%\n",
      "Step :  6500\tLoss: 0.400\tAcc: 90.00%\n",
      "Step :  6600\tLoss: 0.204\tAcc: 95.00%\n",
      "Step :  6700\tLoss: 0.420\tAcc: 91.00%\n",
      "Step :  6800\tLoss: 0.291\tAcc: 94.00%\n",
      "Step :  6900\tLoss: 0.228\tAcc: 94.00%\n",
      "Step :  7000\tLoss: 0.242\tAcc: 91.00%\n",
      "Step :  7100\tLoss: 0.313\tAcc: 90.00%\n",
      "Step :  7200\tLoss: 0.158\tAcc: 94.00%\n",
      "Step :  7300\tLoss: 0.175\tAcc: 95.00%\n",
      "Step :  7400\tLoss: 0.098\tAcc: 98.00%\n",
      "Step :  7500\tLoss: 0.220\tAcc: 94.00%\n",
      "Step :  7600\tLoss: 0.443\tAcc: 92.00%\n",
      "Step :  7700\tLoss: 0.266\tAcc: 94.00%\n",
      "Step :  7800\tLoss: 0.189\tAcc: 94.00%\n",
      "Step :  7900\tLoss: 0.243\tAcc: 92.00%\n",
      "Step :  8000\tLoss: 0.272\tAcc: 91.00%\n",
      "Step :  8100\tLoss: 0.345\tAcc: 90.00%\n",
      "Step :  8200\tLoss: 0.266\tAcc: 93.00%\n",
      "Step :  8300\tLoss: 0.276\tAcc: 95.00%\n",
      "Step :  8400\tLoss: 0.317\tAcc: 91.00%\n",
      "Step :  8500\tLoss: 0.660\tAcc: 90.00%\n",
      "Step :  8600\tLoss: 0.149\tAcc: 96.00%\n",
      "Step :  8700\tLoss: 0.306\tAcc: 93.00%\n",
      "Step :  8800\tLoss: 0.378\tAcc: 90.00%\n",
      "Step :  8900\tLoss: 0.226\tAcc: 94.00%\n",
      "Step :  9000\tLoss: 0.132\tAcc: 95.00%\n",
      "Step :  9100\tLoss: 0.220\tAcc: 93.00%\n",
      "Step :  9200\tLoss: 0.200\tAcc: 94.00%\n",
      "Step :  9300\tLoss: 0.153\tAcc: 95.00%\n",
      "Step :  9400\tLoss: 0.284\tAcc: 94.00%\n",
      "Step :  9500\tLoss: 0.186\tAcc: 94.00%\n",
      "Step :  9600\tLoss: 0.402\tAcc: 91.00%\n",
      "Step :  9700\tLoss: 0.255\tAcc: 92.00%\n",
      "Step :  9800\tLoss: 0.203\tAcc: 94.00%\n",
      "Step :  9900\tLoss: 0.309\tAcc: 94.00%\n",
      "0.9226\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "##### mnist 데이터셋 #####\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "##### 회귀 구현 #####\n",
    "# x 데이터는 n개 * 784의 행렬로 구성되어 있다.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# x와 행렬곱을 해줘야 하므로 x의 차원수인 784, 결과값은 10가지이다. 784*10 행렬. \n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# 결과 값(1*10)에 더해야 하므로 1*10\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# softmax 함수를 적용하여 결과값을 0~1의 값으로 만든다.\n",
    "# matmul : 행렬곱 함수\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "##### 크로스 엔트로피 모델을 설정 #####\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "c_e = -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])\n",
    "# 크로스 엔트로피의 평균.\n",
    "cross_entropy = tf.reduce_mean(c_e)\n",
    "\n",
    "# 크로스 엔트로피 평균값이 작아지도록 경사하강법을 이용하여 최적화한다.\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cross_entropy)\n",
    "\n",
    "##### 모델 평가 #####\n",
    "# softmax 나온 값 중 가장 높은 값과 실제 데이터 비교해서 맞는지 체크\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# 맞으면 1, 틀리면 0이므로 평균을 내면 정확도가 나온다.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "##### 세션 그래프 생성 및 학습 시작 #####\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 2000번 학습\n",
    "for i in range(10000):\n",
    "    # 학습 데이터셋에서 무작위 100개(batch)를 가져온다\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "    #cost 값과 일치율을 같이 출력.\n",
    "        loss, acc = sess.run([cross_entropy, accuracy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        print(\"Step : {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(i, loss, acc))\n",
    "\n",
    "##### 정확도 출력 #####\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
